{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <CENTER> Project Deep Learning : Realtime Multi Person Pose Estimation</CENTER>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan :\n",
    " #### I- Presentation \n",
    " #### II- Formulation\n",
    " #### III- The architecture's network\n",
    " #### IV- The notebook \n",
    " #### V- Results and applications : Comparison with AlphaPose\n",
    " #### VI- References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## I- Presentation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "  The main goal of Human pose estimation problem is to extract the position of body parts of every person appearing in an image or a video and use these keypoints to build the skeleton representing the person's silhouette. This task does not use any sensors or detection systems but only deep learning networks more precisely convolutional pose machines.\n",
    "  \n",
    "A convolutional pose machine is a sequential architecture that composed of convolutional networks which directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations. \n",
    "The expected output of this deep network is the set of skeletons superimposed on the media input(image, video, gif..), in order to train this network (supervised learning) we use labelled image datasets such as COCO dataset , extended-LSP, FLIC dataset ...\n",
    "In the paper we read, the model was trained under 3 augmented datasets :\n",
    " ##### MPII Human Pose Dataset: \n",
    " 25K images containing 40K people with annotated body joints.\n",
    " ##### FLIC dataset:\n",
    " 5003 images from popular Hollywood movies\n",
    " #####  Leeds Sports Pose (LSP) Dataset:\n",
    " 2000 pose annotated images of mostly sports people gathered from Flickr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## II- Formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order tackle the problem, the paper uses a bottom-up approach which consists of detecting all part in the input (i.e. parts of every person), followed by associating/ grouping parts belonging to distinct persons. \n",
    "The main algorithm used in most of the github's repositories is OpenPose either used with caffe( a deep learning framework) or tensorflow, OpenPose is a real-time multi-person and pre-trained model on both MPII Dataset and LSP dataset, its architecture is based on convolutional pose machines and Part Affinity Fields (PAFs). the model has an Invariant running time to the number of people in the image, which will allow us to test in on both images videos and webcam. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## III- The architecture's network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before expliciting the architecture, here are some steps during the training :\n",
    "\n",
    "- First, the image is passed through a baseline network to extract feature maps. In the paper, the author uses the first 10 layers of VGG-19 model.\n",
    "\n",
    "- Then, the feature maps are processed with multiple stages CNN (with relus) to generate: 1) a set of Part Confidence Maps and 2) a set of Part Affinity Fields (PAFs)\n",
    "\n",
    "- Part Confidence Maps: a set of 2D confidence maps S for body part locations. Each joint location has a map.\n",
    "\n",
    "- Part Affinity Fields (PAFs): a set of 2D vector fields L which encodes the degree of association between parts.\n",
    "\n",
    "- Finally, the Confidence Maps and Part Affinity Fields are processed by a greedy algorithm to obtain the poses for each person in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two-branches: \n",
    "The top branch, shown in beige, predicts the confidence maps of different body parts location such as the right eye, left eye, right elbow and others. The bottom branch, shown in blue, predicts the affinity fields, which represents a degree of association between different body parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-Stage:\n",
    "At the first stage, the network produces an initial set of detection confidence maps S and a set of part affinity fields L. Then, in each subsequent stages, the predictions from both branches in the previous stage, along with the original image features F, are concatenated (represented by the + sign ) and used to produce more refined predictions. In the OpenPose implementation, the final stage t is chosen to be 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stage 1:\n",
    "the network produces a set of detection confidence maps S and a set of part affinity fields L.\n",
    "<img src=\"images/img_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stage t: \n",
    "the predictions from both branches in the previous stage, along with the original image features F , are concatenated and used to produce more refined predictions.\n",
    "<img src=\"images/img_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final S and L are the confidence maps and the part affinity fields (PAFs) that will be further processed by the greedy algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To guide the network to iteratively predict PAFs of body parts in the first branch and confidence maps in the second\n",
    "branch, we apply a loss function at the end of each stage. We use an L2 loss between the estimated predictions and\n",
    "the groundtruth maps and fields\n",
    "<img src=\"images/img_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intermediate supervision at each stage addresses the vanishing gradient problem by replenishing\n",
    "the gradient periodically. The overall objective is\n",
    "<img src=\"images/img_5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### IV- The notebook \n",
    "Here is a demo of the openpose algorithm based on tensorflow (the original version was made on caffe but the github refers to a tensorflow version made by contributors), we can test pose estimation on an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports from keras \n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Activation, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from config_reader import config_reader\n",
    "import scipy\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): \n",
    "    return Activation('relu')(x)\n",
    "\n",
    "def conv(x, nf, ks, name):\n",
    "    x1 = Conv2D(nf, (ks, ks), padding='same', name=name)(x)\n",
    "    return x1\n",
    "\n",
    "def pooling(x, ks, st, name):\n",
    "    x = MaxPooling2D((ks, ks), strides=(st, st), name=name)(x)\n",
    "    return x\n",
    "\n",
    "def vgg_block(x):\n",
    "     \n",
    "    # Block 1\n",
    "    x = conv(x, 64, 3, \"conv1_1\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 64, 3, \"conv1_2\")\n",
    "    x = relu(x)\n",
    "    x = pooling(x, 2, 2, \"pool1_1\")\n",
    "\n",
    "    # Block 2\n",
    "    x = conv(x, 128, 3, \"conv2_1\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv2_2\")\n",
    "    x = relu(x)\n",
    "    x = pooling(x, 2, 2, \"pool2_1\")\n",
    "    \n",
    "    # Block 3\n",
    "    x = conv(x, 256, 3, \"conv3_1\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_2\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_3\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 256, 3, \"conv3_4\")\n",
    "    x = relu(x)    \n",
    "    x = pooling(x, 2, 2, \"pool3_1\")\n",
    "    \n",
    "    # Block 4\n",
    "    x = conv(x, 512, 3, \"conv4_1\")\n",
    "    x = relu(x)    \n",
    "    x = conv(x, 512, 3, \"conv4_2\")\n",
    "    x = relu(x)    \n",
    "    \n",
    "    # Additional non vgg layers\n",
    "    x = conv(x, 256, 3, \"conv4_3_CPM\")\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv4_4_CPM\")\n",
    "    x = relu(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def stage1_block(x, num_p, branch):\n",
    "    \n",
    "    # Block 1        \n",
    "    x = conv(x, 128, 3, \"conv5_1_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv5_2_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 3, \"conv5_3_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, 512, 1, \"conv5_4_CPM_L%d\" % branch)\n",
    "    x = relu(x)\n",
    "    x = conv(x, num_p, 1, \"conv5_5_CPM_L%d\" % branch)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def stageT_block(x, num_p, stage, branch):\n",
    "        \n",
    "    # Block 1        \n",
    "    x = conv(x, 128, 7, \"Mconv1_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv2_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv3_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv4_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 7, \"Mconv5_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, 128, 1, \"Mconv6_stage%d_L%d\" % (stage, branch))\n",
    "    x = relu(x)\n",
    "    x = conv(x, num_p, 1, \"Mconv7_stage%d_L%d\" % (stage, branch))\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create keras model and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"model/keras/model.h5\" # orginal weights converted from caffe\n",
    "\n",
    "input_shape = (None,None,3)\n",
    "\n",
    "img_input = Input(shape=input_shape)\n",
    "\n",
    "# the openpose has 6 stages\n",
    "stages = 6\n",
    "np_branch1 = 38\n",
    "np_branch2 = 19\n",
    "\n",
    "img_normalized = Lambda(lambda x: x / 256 - 0.5)(img_input)  # [-0.5, 0.5]\n",
    "\n",
    "# VGG\n",
    "stage0_out = vgg_block(img_normalized)\n",
    "\n",
    "# stage 1\n",
    "stage1_branch1_out = stage1_block(stage0_out, np_branch1, 1)\n",
    "stage1_branch2_out = stage1_block(stage0_out, np_branch2, 2)\n",
    "x = Concatenate()([stage1_branch1_out, stage1_branch2_out, stage0_out])\n",
    "\n",
    "# stage t >= 2\n",
    "for sn in range(2, stages + 1):\n",
    "    stageT_branch1_out = stageT_block(x, np_branch1, sn, 1)\n",
    "    stageT_branch2_out = stageT_block(x, np_branch2, sn, 2)\n",
    "    if (sn < stages):\n",
    "        x = Concatenate()([stageT_branch1_out, stageT_branch2_out, stage0_out])\n",
    "\n",
    "model = Model(img_input, [stageT_branch1_out, stageT_branch2_out])\n",
    "model.load_weights(weights_path)\n",
    "# summary of the model \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a sample image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import matplotlib\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = 'sample_images/human2.jpg'\n",
    "oriImg = cv2.imread(test_image) # B,G,R order\n",
    "plt.imshow(oriImg[:,:,[2,1,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param, model_params = config_reader()\n",
    "\n",
    "multiplier = [x * model_params['boxsize'] / oriImg.shape[0] for x in param['scale_search']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show sample heatmaps for right elbow and paf for right wrist and right elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 19))\n",
    "paf_avg = np.zeros((oriImg.shape[0], oriImg.shape[1], 38))\n",
    "# first figure shows padded images\n",
    "f, axarr = plt.subplots(1, len(multiplier))\n",
    "f.set_size_inches((20, 5))\n",
    "# second figure shows heatmaps\n",
    "f2, axarr2 = plt.subplots(1, len(multiplier))\n",
    "f2.set_size_inches((20, 5))\n",
    "# third figure shows PAFs\n",
    "f3, axarr3 = plt.subplots(2, len(multiplier))\n",
    "f3.set_size_inches((20, 10))\n",
    "\n",
    "for m in range(len(multiplier)):\n",
    "    scale = multiplier[m]\n",
    "    imageToTest = cv2.resize(oriImg, (0,0), fx=scale, fy=scale, interpolation=cv2.INTER_CUBIC)\n",
    "    imageToTest_padded, pad = util.pad_right_down_corner(imageToTest, model_params['stride'], model_params['padValue'])        \n",
    "    axarr[m].imshow(imageToTest_padded[:,:,[2,1,0]])\n",
    "    axarr[m].set_title('Input image: scale %d' % m)\n",
    "\n",
    "    input_img = np.transpose(np.float32(imageToTest_padded[:,:,:,np.newaxis]), (3,0,1,2)) # required shape (1, width, height, channels) \n",
    "    print(\"Input shape: \" + str(input_img.shape))  \n",
    "\n",
    "    output_blobs = model.predict(input_img)\n",
    "    print(\"Output shape (heatmap): \" + str(output_blobs[1].shape))\n",
    "    \n",
    "    # extract outputs, resize, and remove padding\n",
    "    heatmap = np.squeeze(output_blobs[1]) # output 1 is heatmaps\n",
    "    heatmap = cv2.resize(heatmap, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n",
    "    heatmap = heatmap[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n",
    "    heatmap = cv2.resize(heatmap, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    paf = np.squeeze(output_blobs[0]) # output 0 is PAFs\n",
    "    paf = cv2.resize(paf, (0,0), fx=model_params['stride'], fy=model_params['stride'], interpolation=cv2.INTER_CUBIC)\n",
    "    paf = paf[:imageToTest_padded.shape[0]-pad[2], :imageToTest_padded.shape[1]-pad[3], :]\n",
    "    paf = cv2.resize(paf, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # visualization\n",
    "    axarr2[m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax2 = axarr2[m].imshow(heatmap[:,:,3], alpha=.5) # right elbow\n",
    "    axarr2[m].set_title('Heatmaps (Relb): scale %d' % m)\n",
    "    \n",
    "    axarr3.flat[m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax3x = axarr3.flat[m].imshow(paf[:,:,16], alpha=.5) # right elbow\n",
    "    axarr3.flat[m].set_title('PAFs (x comp. of Rwri to Relb): scale %d' % m)\n",
    "    axarr3.flat[len(multiplier) + m].imshow(oriImg[:,:,[2,1,0]])\n",
    "    ax3y = axarr3.flat[len(multiplier) + m].imshow(paf[:,:,17], alpha=.5) # right wrist\n",
    "    axarr3.flat[len(multiplier) + m].set_title('PAFs (y comp. of Relb to Rwri): scale %d' % m)\n",
    "    \n",
    "    heatmap_avg = heatmap_avg + heatmap / len(multiplier)\n",
    "    paf_avg = paf_avg + paf / len(multiplier)\n",
    "\n",
    "f2.subplots_adjust(right=0.93)\n",
    "cbar_ax = f2.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "_ = f2.colorbar(ax2, cax=cbar_ax)\n",
    "\n",
    "f3.subplots_adjust(right=0.93)\n",
    "cbar_axx = f3.add_axes([0.95, 0.57, 0.01, 0.3])\n",
    "_ = f3.colorbar(ax3x, cax=cbar_axx)\n",
    "cbar_axy = f3.add_axes([0.95, 0.15, 0.01, 0.3])\n",
    "_ = f3.colorbar(ax3y, cax=cbar_axy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap for right knee. Note that the body part is encoded in the 3th channel so in this case right knee is \n",
    "at index 9. All body parts are defined in config: \n",
    "part_str = [nose, neck, Rsho, Relb, Rwri, Lsho, Lelb, Lwri, Rhip, Rkne, Rank, Lhip, Lkne, Lank, Leye, Reye, Lear, Rear, pt19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(oriImg[:,:,[2,1,0]])\n",
    "plt.imshow(heatmap_avg[:,:,9], alpha=.5)\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "cax = matplotlib.pyplot.gca()\n",
    "fig.set_size_inches(20, 20)\n",
    "fig.subplots_adjust(right=0.93)\n",
    "cbar_ax = fig.add_axes([0.95, 0.15, 0.01, 0.7])\n",
    "_ = fig.colorbar(ax2, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paf vectors for right elbow and right wrist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ma\n",
    "U = paf_avg[:,:,16] * -1\n",
    "V = paf_avg[:,:,17]\n",
    "X, Y = np.meshgrid(np.arange(U.shape[1]), np.arange(U.shape[0]))\n",
    "M = np.zeros(U.shape, dtype='bool')\n",
    "M[U**2 + V**2 < 0.5 * 0.5] = True\n",
    "U = ma.masked_array(U, mask=M)\n",
    "V = ma.masked_array(V, mask=M)\n",
    "\n",
    "# 1\n",
    "plt.figure()\n",
    "plt.imshow(oriImg[:,:,[2,1,0]], alpha = .5)\n",
    "s = 5\n",
    "Q = plt.quiver(X[::s,::s], Y[::s,::s], U[::s,::s], V[::s,::s], \n",
    "               scale=50, headaxislength=4, alpha=.5, width=0.001, color='r')\n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise all detected body parts. Note that we use peaks in heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage.filters import gaussian_filter\n",
    "all_peaks = []\n",
    "peak_counter = 0\n",
    "\n",
    "for part in range(19-1):\n",
    "    map_ori = heatmap_avg[:,:,part]\n",
    "    map = gaussian_filter(map_ori, sigma=3)\n",
    "    \n",
    "    map_left = np.zeros(map.shape)\n",
    "    map_left[1:,:] = map[:-1,:]\n",
    "    map_right = np.zeros(map.shape)\n",
    "    map_right[:-1,:] = map[1:,:]\n",
    "    map_up = np.zeros(map.shape)\n",
    "    map_up[:,1:] = map[:,:-1]\n",
    "    map_down = np.zeros(map.shape)\n",
    "    map_down[:,:-1] = map[:,1:]\n",
    "    \n",
    "    peaks_binary = np.logical_and.reduce((map>=map_left, map>=map_right, map>=map_up, map>=map_down, map > param['thre1']))\n",
    "    peaks = list(zip(np.nonzero(peaks_binary)[1], np.nonzero(peaks_binary)[0])) # note reverse\n",
    "    peaks_with_score = [x + (map_ori[x[1],x[0]],) for x in peaks]\n",
    "    id = range(peak_counter, peak_counter + len(peaks))\n",
    "    peaks_with_score_and_id = [peaks_with_score[i] + (id[i],) for i in range(len(id))]\n",
    "\n",
    "    all_peaks.append(peaks_with_score_and_id)\n",
    "    peak_counter += len(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find connection in the specified sequence, center 29 is in the position 15\n",
    "limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "# the middle joints heatmap correpondence\n",
    "mapIdx = [[31,32], [39,40], [33,34], [35,36], [41,42], [43,44], [19,20], [21,22], \\\n",
    "          [23,24], [25,26], [27,28], [29,30], [47,48], [49,50], [53,54], [51,52], \\\n",
    "          [55,56], [37,38], [45,46]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_all = []\n",
    "special_k = []\n",
    "mid_num = 10\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    score_mid = paf_avg[:,:,[x-19 for x in mapIdx[k]]]\n",
    "    candA = all_peaks[limbSeq[k][0]-1]\n",
    "    candB = all_peaks[limbSeq[k][1]-1]\n",
    "    nA = len(candA)\n",
    "    nB = len(candB)\n",
    "    indexA, indexB = limbSeq[k]\n",
    "    if(nA != 0 and nB != 0):\n",
    "        connection_candidate = []\n",
    "        for i in range(nA):\n",
    "            for j in range(nB):\n",
    "                vec = np.subtract(candB[j][:2], candA[i][:2])\n",
    "                norm = math.sqrt(vec[0]*vec[0] + vec[1]*vec[1])\n",
    "                # failure case when 2 body parts overlaps\n",
    "                if norm == 0:\n",
    "                    continue\n",
    "                vec = np.divide(vec, norm)\n",
    "                \n",
    "                startend = list(zip(np.linspace(candA[i][0], candB[j][0], num=mid_num), \\\n",
    "                               np.linspace(candA[i][1], candB[j][1], num=mid_num)))\n",
    "                \n",
    "                vec_x = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 0] \\\n",
    "                                  for I in range(len(startend))])\n",
    "                vec_y = np.array([score_mid[int(round(startend[I][1])), int(round(startend[I][0])), 1] \\\n",
    "                                  for I in range(len(startend))])\n",
    "\n",
    "                score_midpts = np.multiply(vec_x, vec[0]) + np.multiply(vec_y, vec[1])\n",
    "                score_with_dist_prior = sum(score_midpts)/len(score_midpts) + min(0.5*oriImg.shape[0]/norm-1, 0)\n",
    "                criterion1 = len(np.nonzero(score_midpts > param['thre2'])[0]) > 0.8 * len(score_midpts)\n",
    "                criterion2 = score_with_dist_prior > 0\n",
    "                if criterion1 and criterion2:\n",
    "                    connection_candidate.append([i, j, score_with_dist_prior, score_with_dist_prior+candA[i][2]+candB[j][2]])\n",
    "\n",
    "        connection_candidate = sorted(connection_candidate, key=lambda x: x[2], reverse=True)\n",
    "        connection = np.zeros((0,5))\n",
    "        for c in range(len(connection_candidate)):\n",
    "            i,j,s = connection_candidate[c][0:3]\n",
    "            if(i not in connection[:,3] and j not in connection[:,4]):\n",
    "                connection = np.vstack([connection, [candA[i][3], candB[j][3], s, i, j]])\n",
    "                if(len(connection) >= min(nA, nB)):\n",
    "                    break\n",
    "\n",
    "        connection_all.append(connection)\n",
    "    else:\n",
    "        special_k.append(k)\n",
    "        connection_all.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last number in each row is the total parts number of that person\n",
    "# the second last number in each row is the score of the overall configuration\n",
    "subset = -1 * np.ones((0, 20))\n",
    "candidate = np.array([item for sublist in all_peaks for item in sublist])\n",
    "\n",
    "for k in range(len(mapIdx)):\n",
    "    if k not in special_k:\n",
    "        partAs = connection_all[k][:,0]\n",
    "        partBs = connection_all[k][:,1]\n",
    "        indexA, indexB = np.array(limbSeq[k]) - 1\n",
    "\n",
    "        for i in range(len(connection_all[k])): #= 1:size(temp,1)\n",
    "            found = 0\n",
    "            subset_idx = [-1, -1]\n",
    "            for j in range(len(subset)): #1:size(subset,1):\n",
    "                if subset[j][indexA] == partAs[i] or subset[j][indexB] == partBs[i]:\n",
    "                    subset_idx[found] = j\n",
    "                    found += 1\n",
    "            \n",
    "            if found == 1:\n",
    "                j = subset_idx[0]\n",
    "                if(subset[j][indexB] != partBs[i]):\n",
    "                    subset[j][indexB] = partBs[i]\n",
    "                    subset[j][-1] += 1\n",
    "                    subset[j][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "            elif found == 2: # if found 2 and disjoint, merge them\n",
    "                j1, j2 = subset_idx\n",
    "                print (\"found = 2\")\n",
    "                membership = ((subset[j1]>=0).astype(int) + (subset[j2]>=0).astype(int))[:-2]\n",
    "                if len(np.nonzero(membership == 2)[0]) == 0: #merge\n",
    "                    subset[j1][:-2] += (subset[j2][:-2] + 1)\n",
    "                    subset[j1][-2:] += subset[j2][-2:]\n",
    "                    subset[j1][-2] += connection_all[k][i][2]\n",
    "                    subset = np.delete(subset, j2, 0)\n",
    "                else: # as like found == 1\n",
    "                    subset[j1][indexB] = partBs[i]\n",
    "                    subset[j1][-1] += 1\n",
    "                    subset[j1][-2] += candidate[partBs[i].astype(int), 2] + connection_all[k][i][2]\n",
    "\n",
    "            # if find no partA in the subset, create a new subset\n",
    "            elif not found and k < 17:\n",
    "                row = -1 * np.ones(20)\n",
    "                row[indexA] = partAs[i]\n",
    "                row[indexB] = partBs[i]\n",
    "                row[-1] = 2\n",
    "                row[-2] = sum(candidate[connection_all[k][i,:2].astype(int), 2]) + connection_all[k][i][2]\n",
    "                subset = np.vstack([subset, row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete some rows of subset which has few parts occur\n",
    "deleteIdx = [];\n",
    "for i in range(len(subset)):\n",
    "    if subset[i][-1] < 4 or subset[i][-2]/subset[i][-1] < 0.4:\n",
    "        deleteIdx.append(i)\n",
    "subset = np.delete(subset, deleteIdx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
    "cmap = matplotlib.cm.get_cmap('hsv')\n",
    "\n",
    "canvas = cv2.imread(test_image) # B,G,R order\n",
    "\n",
    "for i in range(18):\n",
    "    rgba = np.array(cmap(1 - i/18. - 1./36))\n",
    "    rgba[0:3] *= 255\n",
    "    for j in range(len(all_peaks[i])):\n",
    "        cv2.circle(canvas, all_peaks[i][j][0:2], 4, colors[i], thickness=-1)\n",
    "\n",
    "to_plot = cv2.addWeighted(oriImg, 0.3, canvas, 0.7, 0)\n",
    "plt.imshow(to_plot[:,:,[2,1,0]])\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link body parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize 2\n",
    "stickwidth = 4\n",
    "\n",
    "for i in range(17):\n",
    "    for n in range(len(subset)):\n",
    "        index = subset[n][np.array(limbSeq[i])-1]\n",
    "        if -1 in index:\n",
    "            continue\n",
    "        cur_canvas = canvas.copy()\n",
    "        Y = candidate[index.astype(int), 0]\n",
    "        X = candidate[index.astype(int), 1]\n",
    "        mX = np.mean(X)\n",
    "        mY = np.mean(Y)\n",
    "        length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
    "        angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
    "        polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n",
    "        cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
    "        canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "        \n",
    "plt.imshow(canvas[:,:,[2,1,0]])\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(12, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application on youtube videos : Comparison between OpenPose and AlphaPose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While OpenPose is a bottom-up model, AlphaPose follow a top-down approach which means that the algorithm tracks first each person separately in the image/video... and then detects its body parts and then builds the skeletons.\n",
    "In order to compare between these two approaches, we chose to test the pre-trained model on youtube videos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the links to the two interactice google colab notebooks for both OpenPose and AlphaPose.\n",
    "In the first notebook, the openpose model caffe's version is applied on youtube videos only based on its id. we chose google colab plateform because we needed to execute a bunch of linux line commands that weren't executable in our computers, in order to build openpose library.\n",
    "In the second notebook, we use AlphaPose for the same task.\n",
    "\n",
    "OpenPose : https://colab.research.google.com/drive/1quTmKNI3NwMGFlAPGM-6ffisn49T3Wb7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlphaPose: https://colab.research.google.com/drive/1Q8F5D7TVgMmQZbU1rGhdtFW-dcBfeIbG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tested both models on the same video, here are some pics extracted from the output video for both models.\n",
    "The pics were extracted at the 5th second of the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenPose estimation :\n",
    "<img src=\"images/img_6.png\">\n",
    "\n",
    "### AlphaPose estimation :\n",
    "<img src=\"images/img_7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the openpose model detects more keypoints corresponding to foot part than alphapose,the links creating the skeleton are thiner in AlphaPose output therefore more accurates. Contrary to OpenPose, AlphaPose skeletons don't have a link in the hips part, which could be a disadvantage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Points of difference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on AlphaPose official paper here are the accuracies corresponding to testing on MPII dataset for different models :\n",
    "<img src=\"images/img_8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that Alphapose model is doing better comparing to OpenPose to detect body parts, Another major difference between the two pose estimation libraries is that when OpenPose's runtime is invariant of the number of person in the image, AlphaPose's runtime grows linearly with the number of people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/araffin/tf-pose-estimation\n",
    "- https://github.com/araffin/tf-pose-estimation\n",
    "- https://github.com/ZheC/Realtime_Multi-Person_Pose_Estimation\n",
    "- https://arxiv.org/abs/1602.00134\n",
    "- https://github.com/michalfaber/keras_Realtime_Multi-Person_Pose_Estimation"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
